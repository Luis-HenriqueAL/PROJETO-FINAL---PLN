{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk\n",
        "%pip install tensorflow\n",
        "%pip install yake\n",
        "%pip install spacy\n",
        "%pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cT8LpSm-qjc",
        "outputId": "0a4eb1ac-1d61-464d-d31b-95b75a1fbc39"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: yake in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.8.10)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.22.4)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.1)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (0.11.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gANmIuQPlohU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk"
      ],
      "metadata": {
        "id": "nsqnrjGNujpI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "dON060L0_3PM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "serACEsX_4EF"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake"
      ],
      "metadata": {
        "id": "zD8Eb5II_4sd"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "spacy.cli.download('en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLAD95db_uit",
        "outputId": "1f960ef0-8de3-40aa-f981-fa2b9af925fd"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leitura e pré-processamento dos dados\n",
        "def preprocess_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    text = text.lower()  # Convertendo para minúsculas\n",
        "    return text"
      ],
      "metadata": {
        "id": "0QO8UGaQluKb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição dos hiperparâmetros\n",
        "max_sequence_length = 1000\n",
        "max_words = 10000\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "cs9zgDYvlvd7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregamento e pré-processamento dos dados\n",
        "data = preprocess_data('texto.txt')"
      ],
      "metadata": {
        "id": "npeCZdt-lwtr"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenização dos dados\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts([data])\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1"
      ],
      "metadata": {
        "id": "dkBjalQsmEur"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação das sequências de treinamento\n",
        "input_sequences = []\n",
        "for line in data.split('\\n'):\n",
        "    tokenized = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokenized)):\n",
        "        n_gram_sequence = tokenized[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "metadata": {
        "id": "5QpuxSZHmFzt"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding das sequências de entrada\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre'))"
      ],
      "metadata": {
        "id": "Wp9FIKOhmNH6"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separação dos dados de entrada e saída\n",
        "xs = input_sequences[:, :-1]\n",
        "labels = input_sequences[:, -1]"
      ],
      "metadata": {
        "id": "Hi5-LlFFmOpK"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação dos vetores one-hot para as saídas\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "metadata": {
        "id": "iRMnoUyTmQY6"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação do modelo de rede neural\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, embedding_dim, input_length=max_sequence_length-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "MrULpesamaB6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do modelo\n",
        "model.fit(xs, ys, epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBqH3HR1mb05",
        "outputId": "9d596857-ce63-48fc-c55d-76becbdc3392"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 3s 80ms/step - loss: 5.4133\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 2s 137ms/step - loss: 5.0826\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 2s 156ms/step - loss: 4.9365\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 2s 161ms/step - loss: 4.8960\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 2s 152ms/step - loss: 4.8855\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 2s 149ms/step - loss: 4.8720\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 3s 190ms/step - loss: 4.8498\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 3s 175ms/step - loss: 4.8171\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 2s 166ms/step - loss: 4.7893\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 2s 161ms/step - loss: 4.7177\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 4.6121\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 4.4746\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 2s 106ms/step - loss: 4.3509\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 2s 113ms/step - loss: 4.1967\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 4.0156\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 3.8303\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 3.6410\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 3.4459\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 3.2599\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 3.0524\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 2.8652\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 1s 95ms/step - loss: 2.6902\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 2s 159ms/step - loss: 2.5087\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 2s 100ms/step - loss: 2.3460\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 2.1782\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 2.0271\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 1.8851\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 1.7701\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 1s 79ms/step - loss: 1.6542\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 1.5360\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 1.4281\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 2s 113ms/step - loss: 1.3277\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 2s 99ms/step - loss: 1.2424\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 1.1718\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 1.0945\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 1.0172\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.9514\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 1s 79ms/step - loss: 0.8936\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.8388\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.7845\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 2s 112ms/step - loss: 0.7404\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 2s 102ms/step - loss: 0.6960\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.6579\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.6174\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.5870\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.5519\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.5242\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.4958\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.4687\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 2s 106ms/step - loss: 0.4453\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 2s 108ms/step - loss: 0.4209\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.4008\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.3806\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.3645\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.3457\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 1s 79ms/step - loss: 0.3306\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.3153\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.3030\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 2s 129ms/step - loss: 0.2883\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 2s 125ms/step - loss: 0.2760\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.2638\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.2537\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.2454\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 2s 117ms/step - loss: 0.2351\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 2s 132ms/step - loss: 0.2256\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 2s 160ms/step - loss: 0.2160\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 3s 212ms/step - loss: 0.2086\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.2003\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.1931\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.1858\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.1794\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.1753\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.1677\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 1s 85ms/step - loss: 0.1613\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 2s 127ms/step - loss: 0.1561\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.1514\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.1498\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1424\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.1377\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1331\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1302\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1266\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1215\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 2s 136ms/step - loss: 0.1180\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 1s 84ms/step - loss: 0.1158\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 1s 83ms/step - loss: 0.1119\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1077\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.1057\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1047\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.1006\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 1s 85ms/step - loss: 0.0978\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 1s 85ms/step - loss: 0.0951\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 2s 134ms/step - loss: 0.0936\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.0900\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.0880\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 1s 81ms/step - loss: 0.0858\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 1s 84ms/step - loss: 0.0838\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 1s 80ms/step - loss: 0.0818\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.0815\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 1s 82ms/step - loss: 0.0795\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4db316d540>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Geração de texto a partir do modelo treinado\n",
        "def generate_text(seed_text, next_words, model, max_sequence_length):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "F1Bx8BGEmd6Z"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento do texto a partir do texto gerado pelo modelo\n",
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [[word for word in sentence if word.lower() not in stop_words] for sentence in words]\n",
        "\n",
        "    tagged_words = [pos_tag(sentence, lang='eng') for sentence in words]\n",
        "\n",
        "    return tagged_words"
      ],
      "metadata": {
        "id": "e_Hns7ckt_Cr"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconhecimento de Entidades Nomeadas\n",
        "def extract_named_entities(tagged_words):\n",
        "    entities = []\n",
        "    for sentence in tagged_words:\n",
        "        text = ' '.join([word + '/' + tag for word, tag in sentence])\n",
        "        doc = nlp(text)\n",
        "        for entity in doc.ents:\n",
        "            entities.append((entity.text, entity.label_))\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "C7oW5bMVuA5a"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extração das palavras chave\n",
        "def extract_keywords(text, num_keywords=20):\n",
        "    # Tokenizar o texto em palavras\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Remover stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    # Criar uma matriz TF-IDF\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([' '.join(tokens)])\n",
        "\n",
        "    # Converter a matriz esparsa em uma matriz densa\n",
        "    dense_matrix = tfidf_matrix.toarray()\n",
        "\n",
        "    # Obter os índices das palavras com as maiores pontuações TF-IDF\n",
        "    top_indices = dense_matrix.argsort()[0, ::-1][:num_keywords]\n",
        "\n",
        "    # Obter as palavras-chave correspondentes aos índices\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    keywords = [feature_names[idx] for idx in top_indices]\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "pc6hDL8quCQR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação do resumo\n",
        "def create_summary(text, num_sentences=3):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum()]\n",
        "\n",
        "    word_frequencies = FreqDist(words)\n",
        "\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentences:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if len(sentence.split(' ')) < 30:\n",
        "                    if sentence not in sentence_scores:\n",
        "                        sentence_scores[sentence] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:num_sentences]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "2gR6U1IGuDqJ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega recursos inglês do scypi\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7HKX5PmGuFzK"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando o texto\n",
        "# Parâmetros: seed para geração do texto, quantidade de palavras seguintes,\n",
        "# modelo, máximo de sequência de palavras possíveis\n",
        "generated_text = generate_text(\"In Brazil\", 100, model, max_sequence_length)\n",
        "display(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "43HRLqPtoPmz",
        "outputId": "a7272027-6f26-43eb-82a4-5d34c55bdd38"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\"In Brazil in addition to political polarization and the elections 2018 was also marked by corruption scandals that continued to shake the country operation car wash initiated in 2014 revealed a vast scheme of embezzlement of public funds involving high ranking politicians and businessmen this operation had a profound impact on brazilian society generating outrage and a demand for changes in the political system system was considered one of party party party party party party party party party party party the workers' party party party party party party party party party party the workers' party party party party party party party party\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento do texto gerado para extração de entidades nomeadas,\n",
        "# palavras-chave e criação do resumo a partir do texto\n",
        "\n",
        "tagged_words = preprocess_text(generated_text)\n",
        "entities = extract_named_entities(tagged_words)\n",
        "key_words = extract_keywords(generated_text)\n",
        "summary = create_summary(generated_text)"
      ],
      "metadata": {
        "id": "gOYbqfJiuTQB"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as entidades nomeadas\n",
        "print(\"Entidades Nomeadas:\")\n",
        "for entity in entities:\n",
        "    print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfEmhV2SuVmQ",
        "outputId": "f8d1edf4-5282-441c-bdf8-a69ad9aff546"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entidades Nomeadas:\n",
            "('Brazil', 'GPE')\n",
            "('NNP', 'ORG')\n",
            "('2018', 'DATE')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('2014', 'DATE')\n",
            "('NNS', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('NNS', 'PERSON')\n",
            "('VBG', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('NNS', 'ORG')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as palavras-chave\n",
        "print(\"\\nPalavras-chave:\")\n",
        "for keyword in key_words:\n",
        "    print(keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6nyJcYzuW1g",
        "outputId": "12c700b6-1d2e-454f-bba9-b2be5787f192"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palavras-chave:\n",
            "party\n",
            "workers\n",
            "system\n",
            "operation\n",
            "political\n",
            "continued\n",
            "funds\n",
            "embezzlement\n",
            "elections\n",
            "demand\n",
            "country\n",
            "corruption\n",
            "considered\n",
            "high\n",
            "changes\n",
            "car\n",
            "businessmen\n",
            "brazilian\n",
            "brazil\n",
            "also\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 12: Imprimir o resumo\n",
        "print(\"\\nResumo:\")\n",
        "display(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "aHcAcP8g0nvW",
        "outputId": "c5adbbbe-d495-4e10-87c5-14b0c281d617"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento do texto gerado para extração de entidades nomeadas,\n",
        "# palavras-chave e criação do resumo a partir do documento de texto inicial\n",
        "\n",
        "txt_tagged_words = preprocess_text(data)\n",
        "txt_entities = extract_named_entities(txt_tagged_words)\n",
        "txt_key_words = extract_keywords(data)\n",
        "txt_summary = create_summary(data)"
      ],
      "metadata": {
        "id": "4l3iWWfhT4KX"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as entidades nomeadas\n",
        "print(\"Entidades Nomeadas:\")\n",
        "for entity in txt_entities:\n",
        "    print(entity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHbh1HErT7w-",
        "outputId": "c0d1badb-cd90-4442-e9ff-8695a951540d"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entidades Nomeadas:\n",
            "('JJ 2018', 'DATE')\n",
            "('NNS', 'ORG')\n",
            "('VBZ', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('society/NN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBP', 'ORG')\n",
            "('NNS', 'PERSON')\n",
            "('NNS', 'ORG')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBP', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'PERSON')\n",
            "('NNS', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBP', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBP', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('NNS', 'PERSON')\n",
            "('NNS', 'ORG')\n",
            "('VBP', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('2018', 'DATE')\n",
            "('VBN', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('JJ 2014', 'DATE')\n",
            "('NNS', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('brazil', 'GPE')\n",
            "('VBP', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBP 2018', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBP', 'ORG')\n",
            "('NNS', 'ORG')\n",
            "('VBG', 'ORG')\n",
            "('brazil', 'GPE')\n",
            "('VBG', 'ORG')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as palavras-chave\n",
        "print(\"\\nPalavras-chave:\")\n",
        "for keyword in txt_key_words:\n",
        "    print(keyword)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jpf_T9smUBEu",
        "outputId": "79f38d74-a04a-410f-dda0-e9a8f4ad6897"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Palavras-chave:\n",
            "political\n",
            "elections\n",
            "corruption\n",
            "country\n",
            "impact\n",
            "marked\n",
            "debates\n",
            "brazil\n",
            "support\n",
            "politics\n",
            "future\n",
            "social\n",
            "candidates\n",
            "one\n",
            "bolsonaro\n",
            "significant\n",
            "polarization\n",
            "events\n",
            "jair\n",
            "presidential\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_doc = create_summary(data)\n",
        "print(\"\\nResumo do documento:\")\n",
        "display(summary_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Z2s9-o9dBWqB",
        "outputId": "73b79beb-3fc5-4075-f7c4-a581649ff226"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo do documento:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'the results of these events and political actions had a lasting impact on brazil, shaping the political and social landscape in the subsequent years. the politics in brazil in 2018 were marked by a series of events and occurrences that had a significant impact on the country. it was a year of presidential elections and intense debates about the political, social, and economic future of the nation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4bsKXmLCEA_"
      },
      "execution_count": 99,
      "outputs": []
    }
  ]
}